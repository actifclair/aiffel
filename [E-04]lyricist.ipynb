{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e3235c",
   "metadata": {},
   "source": [
    "# 1. ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "/aiffel/lyricist/data/lyrics/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e832ae9b",
   "metadata": {},
   "source": [
    "# 2. ë°ì´í„° ì½ì–´ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e29d2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "239a40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faa70415",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 187088\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# ì—¬ëŸ¬ê°œì˜ txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ raw_corpus ì— ë‹´ìŒ\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdac440b",
   "metadata": {},
   "source": [
    "# 3. ë°ì´í„° ë‹¤ë“¬ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe9024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()  # ì†Œë¬¸ì, ì–‘ìª½ê³µë°± ì œê±°\n",
    "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence)  # íŠ¹ìˆ˜ë¬¸ì ì–‘ìª½ì— ê³µë°±ì„ ì¶”ê°€\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)  # ê³µë°± íŒ¨í„´ì„ ë§Œë‚˜ë©´ ìŠ¤í˜ì´ìŠ¤ 1ê°œë¡œ ì¹˜í™˜\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", sentence)  # íŒ¨í„´ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ì(ê³µë°±ë¬¸ìê¹Œì§€ë„)ë¥¼ ìŠ¤í˜ì´ìŠ¤ 1ê°œë¡œ ì¹˜í™˜\n",
    "    sentence = sentence.strip()  # ì–‘ìª½ ê³µë°± ì œê±°\n",
    "    sentence = '<start> ' + sentence + ' <end>'\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0598e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []  # í˜•íƒœ : ['<start> i m begging of you please don t take my man <end>', ...] length - 175986\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    tmp = preprocess_sentence(sentence)\n",
    "    if len(tmp.split()) > 15: continue\n",
    "    corpus.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83cdd439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus):\n",
    "    # num_words:ì „ì²´ ë‹¨ì–´ì˜ ê°œìˆ˜, filters:ë³„ë„ë¡œ ì „ì²˜ë¦¬ ë¡œì§ì„ ì¶”ê°€, oov_token: out-of-vocabulary ì‚¬ì „ì— ì—†ì—ˆë˜ ë‹¨ì–´ëŠ” ì–´ë–¤ í† í°ìœ¼ë¡œ ëŒ€ì²´í• ì§€\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=12000, filters=' ', oov_token=\"<unk>\")\n",
    "    tokenizer.fit_on_texts(corpus)  # corpusë¡œë¶€í„° Tokenizerê°€ ì‚¬ì „ì„ ìë™êµ¬ì¶•\n",
    "\n",
    "    # tokenizerë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ì— ì…ë ¥í•  ë°ì´í„°ì…‹ êµ¬ì¶•(Tensorë¡œ ë³€í™˜)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    # ì…ë ¥ ë°ì´í„° ì‹œí€€ìŠ¤ ê¸¸ì´ ë§ì¶°ì£¼ê¸° - padding\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)\n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4272a583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# ë‹¨ì–´ ì‚¬ì „ì´ ì–´ë–»ê²Œ êµ¬ì¶•ë˜ì—ˆëŠ”ì§€ í™•ì¸ ë°©ë²•\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11844bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124981, 14)\n",
      "Target Train: (124981, 14)\n"
     ]
    }
   ],
   "source": [
    "# 3. í‰ê°€ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "\"\"\"\n",
    "x_train : ì†ŒìŠ¤ ë¬¸ì¥, í˜•ì‹(<start> ë¬¸ì¥), ì¦‰ <end>ë¥¼ ì‚­ì œ\n",
    "y_train : íƒ€ê²Ÿ ë¬¸ì¥, í˜•ì‹(ë¬¸ì¥ <end>), ì¦‰ <start>ë¥¼ ì‚­ì œ\n",
    "ë‹¨ì–´ì¥ì˜ í¬ê¸°ëŠ” 12,000 ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”! ì´ ë°ì´í„°ì˜ 20%ë¥¼ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”!\n",
    "\"\"\"\n",
    "src_input = tensor[:, :-1]  # tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ìƒì„±. ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ë‹¤.\n",
    "tgt_input = tensor[:, 1:]  # tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„± -> ë¬¸ì¥ ê¸¸ì´ëŠ” 14ê°€ ë¨\n",
    "\n",
    "# train dataë¥¼ train, validë¡œ ë‚˜ëˆˆë‹¤.(ë¹„ìœ¨ 80:20) ë§Œì•½ í•™ìŠµë°ì´í„° ê°œìˆ˜ê°€ 124960ë³´ë‹¤ í¬ë‹¤ë©´ ìœ„ Step 3.ì˜ ë°ì´í„° ì •ì œ ê³¼ì •ì„ ë‹¤ì‹œ ê²€í† \n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=20)\n",
    "print(\"Source Train:\", enc_train.shape)  # (124960, 14)  # í˜„ì¬ (124981, 14)\n",
    "print(\"Target Train:\", dec_train.shape)  # (124960, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549eeed4",
   "metadata": {},
   "source": [
    "# 4. ëª¨ë¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20577396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)  # ì…ë ¥ëœ í…ì„œì—ëŠ” ë‹¨ì–´ì‚¬ì „ì˜ ì¸ë±ìŠ¤ê°€ ë“¤ì–´ìˆëŠ”ë°, ì´ ì¸ë±ìŠ¤ ê°’ì„ í•´ë‹¹ ì¸ë±ìŠ¤ ë²ˆì§¸ì˜ ì›Œë“œ ë²¡í„°ë¡œ ë°”ê¿”ì¤€ë‹¤.\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2289d048",
   "metadata": {},
   "source": [
    "# 5. ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba5ce0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3906/3906 [==============================] - 147s 36ms/step - loss: 3.0829 - val_loss: 2.8246\n",
      "Epoch 2/10\n",
      "3906/3906 [==============================] - 141s 36ms/step - loss: 2.6598 - val_loss: 2.6250\n",
      "Epoch 3/10\n",
      "3906/3906 [==============================] - 141s 36ms/step - loss: 2.3823 - val_loss: 2.5020\n",
      "Epoch 4/10\n",
      "3906/3906 [==============================] - 141s 36ms/step - loss: 2.1297 - val_loss: 2.4289\n",
      "Epoch 5/10\n",
      "3906/3906 [==============================] - 141s 36ms/step - loss: 1.9076 - val_loss: 2.3836\n",
      "Epoch 6/10\n",
      "3906/3906 [==============================] - 141s 36ms/step - loss: 1.7175 - val_loss: 2.3594\n",
      "Epoch 7/10\n",
      "3906/3906 [==============================] - 141s 36ms/step - loss: 1.5575 - val_loss: 2.3553\n",
      "Epoch 8/10\n",
      "3906/3906 [==============================] - 141s 36ms/step - loss: 1.4241 - val_loss: 2.3646\n",
      "Epoch 9/10\n",
      "3906/3906 [==============================] - 141s 36ms/step - loss: 1.3159 - val_loss: 2.3915\n",
      "Epoch 10/10\n",
      "3906/3906 [==============================] - 141s 36ms/step - loss: 1.2297 - val_loss: 2.4186\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=10, validation_data=(enc_val, dec_val))\n",
    "\n",
    "\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œ ì…ë ¥ë°›ì€ init_sentenceë„ ì¼ë‹¨ í…ì„œë¡œ ë³€í™˜\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor)  # ì…ë ¥ë°›ì€ ë¬¸ì¥ì˜ í…ì„œë¥¼ ì…ë ¥\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]  # ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë§ˆì§€ë§‰ ë‹¨ì–´ê°€ ë°”ë¡œ ìƒˆë¡­ê²Œ ìƒì„±í•œ ë‹¨ì–´ê°€ ë¨\n",
    "\n",
    "        # ëª¨ë¸ì´ ìƒˆë¡­ê²Œ ì˜ˆì¸¡í•œ ë‹¨ì–´ë¥¼ ì…ë ¥ ë¬¸ì¥ì˜ ë’¤ì— ë¶™ì—¬ì¤Œ\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # ëª¨ë¸ì´ <end>ë¥¼ ì˜ˆì¸¡í–ˆê±°ë‚˜, max_lenì— ë„ë‹¬í•˜ì§€ ì•Šì•˜ë‹¤ë©´  while ë£¨í”„ë¥¼ ë˜ ëŒë©´ì„œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # ìƒì„±ëœ tensor ì•ˆì— ìˆëŠ” word indexë¥¼ tokenizer.index_word ì‚¬ì „ì„ í†µí•´ ì‹¤ì œ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d9d1e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i love you <end> \n"
     ]
    }
   ],
   "source": [
    "test_sen = generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)\n",
    "print(test_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206db2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f014446b",
   "metadata": {},
   "source": [
    "# [E-04] íšŒê³ \n",
    "\n",
    "ë°ì´í„°ë¥¼ ë‹¤ìš´ ë°›ê³ , ì½ê³ , ë‹¤ë“¬ê³ , ëª¨ë¸ ìƒì„± í›„ í•™ìŠµê¹Œì§€ëŠ” ì¢‹ì•˜ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ë²ˆ explorationì€ LMSë…¸ë“œì™€ í•¨ê»˜ êµ¬ê¸€ë§ì„ í•˜ë©° í•´ë‚¼ ì¤„ ì•Œì•˜ìŠµë‹ˆë‹¤ğŸ˜‚\n",
    "\n",
    "ê·¸ëŸ°ë° ì§„ì§œ ë§ˆì§€ë§‰ì—... ê³¨ì¸ ì§€ì ì„ ëˆˆì•ì— ë†”ë‘ê³  'i love'ê°€ ë“¤ì–´ê°„ ë¬¸ì¥ì„ ì¶œë ¥í•˜ëŠ” ë¶€ë¶„ì—ì„œ ë­”ê°€ê°€ ì‚ê±±ê±°ë¦¬ê¸° ì‹œì‘í–ˆëŠ”ë°, ê·¸ ë­”ê°€ë¥¼ ì•Œ ìˆ˜ ì—†ì–´ì„œ êµ¬ê¸€ë§ì„ í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìë£Œë¥¼ ì°¾ì•„ë³´ë©´ì„œ ì½”ë“œë¥¼ ë°”ê¿”ë³´ê³  ì‹¤í–‰í•´ë³´ê³  ê´œì°®ì•„ì„œ ë„˜ì–´ê°”ë‹¤ ì‹¶ìœ¼ë©´ ê·¸ ë‹¤ìŒì´ ë˜ ë¬¸ì œê³ ğŸ˜… í•œ ë°œì§ ë‘ ë°œì§ ë‹¤ê°€ê°€ë©´ ì„¸ ë°œì§ ë©€ì–´ì ¸ì„œ ì–´ë””ê°€ ë¬¸ì œì¸ì§€, ì–´ë””ì¯¤ì— ìˆëŠ”ì§€ ëª°ë¼ì„œ ë‹¤ì‹œ ì²˜ìŒë¶€í„° ì§„í–‰í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "(bgmğŸµ í•œ ë°œì§ ë‘ ë°œì§ ğŸ¤ì˜¤ë§ˆì´ê±¸)\n",
    "\n",
    "ë‹¤ì‹œ ì²˜ìŒ ë¶€í„° ì‹œì‘í•˜ë©´ì„œ ìì‹ ìˆê²Œ ë°ì´í„° ì •ì œ ë¶€ë¶„ê¹Œì§€ ë§ˆì¹˜ê³ , ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ìˆœê°„ë¶€í„° ì‘ë…„ì— í•´ë‹¹ explorationì„ ì§„í–‰í•˜ì…¨ë˜ ë¶„ì˜ ë¸”ë¡œê·¸ë¥¼ ì°¸ê³ (ë¼ê³  ì“°ê³  ctrl+c, ctrl+v)í•´ explorationì´ ì™„ì„±ë  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "https://velog.io/@nameunzz/%EC%9E%91%EC%82%AC%EA%B0%80-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%A7%8C%EB%93%A4%EA%B8%B0\n",
    "\n",
    "ê°€ì‚¬ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆê³  í…ìŠ¤íŠ¸ ì œë„ˆë ˆì´ì…˜ ê²°ê³¼ê°€ ê·¸ëŸ´ë“¯í•œ ë¬¸ì¥ìœ¼ë¡œ ìƒì„± ë˜ì—ˆëŠ” ê²ƒ ê°™ì€ë°, ì™œ ë¬¸ì¥ì´ í•˜ë‚˜ ë°–ì— ì¶œë ¥ì´ ì•ˆ ë˜ì—ˆì„ê¹Œìš”?\n",
    "\n",
    "ì´ê±´ ì œê°€ ì²˜ìŒì— ë°ì´í„°ë¥¼ ë‹¤ìš´ë°›ì„ ë•Œ í´ë¼ìš°ë“œ ì‰˜ì—ì„œ ì••ì¶•ì„ í‘¸ëŠ” ê³¼ì •ì—ì„œ ì œê°€ ë“£ê¸° ì‹«ì€ ê°€ìˆ˜ëŠ” ì••ì¶•í•´ì œë¥¼ ì•ˆ í•´ì„œ ë‚˜ì˜¨ ê²°ê³¼ ê°™ìŠµë‹ˆë‹¤ğŸ˜…\n",
    "\n",
    "ê·¸ë¦¬ê³  í…ìŠ¤íŠ¸ ìƒì„±ëª¨ë¸ì˜ validation lossê°€ 2.2 ì´í•˜ë¡œ ë‚®ì•„ì§€ì§€ëŠ” ì•Šì•˜ìŠµë‹ˆë‹¤. ì—í¬í¬ë¥¼ 10í• ë‹¹í•˜ê³  ëŒë ¸ì„ë•Œ ì²˜ìŒì—ëŠ” LMSì— ìˆë˜ ê·¸ëŒ€ë¡œ embedding_size = 256, hidden_size = 1024ë¥¼ ì£¼ì—ˆì„ ë•Œ 2.2ì´ìƒìœ¼ë¡œ ë‚˜ì™€ì„œ ì¼ê¾¼ì„ 3000ê¹Œì§€ ëŠ˜ë ¸ëŠ”ë° ìµœì†Œ 2.3Rkwl sofurkrh ì—í¬í¬ 7ë²ˆì§¸ë¥¼ ë„˜ì–´ê°€ë©´ ë‹¤ì‹œ validation loss ê°’ì´ ì˜¬ë¼ê°”ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ê²°êµ­ ë‹¤ì‹œ ì¼ê¾¼ì„ ì¤„ì´ê³  ì œìë¦¬ë¡œ ëŒì•„ì™”ìŠµë‹ˆë‹¤ğŸ˜­\n",
    "\n",
    "validation loss ì¤„ì´ëŠ” ë°©ë²•ì„ ê²€ìƒ‰í•´ë´¤ëŠ”ë° ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸ì˜ ê°•ì œì„±(ë³µì¡í•˜ì§€ ì•ŠëŠ”ì§€ ë“±), í•™ìŠµì†ë„ ì¤„ì´ê¸° ë“±ì´ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
    "exploration 4ë¥¼ ì²˜ìŒ ë¶€í„° ë‹¤ì‹œ í•  ë•Œ ìµœëŒ€í•œ ë¶ˆí”¼ìš” í•´ë³´ì´ëŠ” ê²ƒë“¤(ë°°ì¹˜ í™•ì¸ ë“±)ì„ ì œì™¸í•˜ê³  ë‹¤ì‹œ ë§Œë“¤ì–´ì„œ ì €ëŠ” ë³µì¡í•˜ì§€ ì•Šë‹¤ê³  ìƒê°í–ˆëŠ”ë°... í”„ë¡œê·¸ë¨ì€ ë³µì¡í•˜ë‹¤ê³  ìƒê°í•˜ëŠ”ì§€ validation loss ê°’ì´ ì¤„ì–´ë“¤ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì´ë²ˆ ë…¸ë“œë¥¼ ì‹¤í–‰í•˜ê³  í•™ìŠµìë£Œë¥¼ ì°¾ì•„ë³´ë©´ì„œ ì œê°€ í”„ë¡œê·¸ë¨ì„ ì–´ë–»ê²Œ ë§Œë“¤ê³  ìˆëŠ”ì§€, ì˜ ì´í•´í•˜ë©´ì„œ ë§Œë“¤ê³  ìˆëŠ”ì§€ ë‹¤ì‹œ ìƒê°í•´ë³´ëŠ” ì‹œê°„ì´ì—ˆë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
    "ê·¸ë˜ë„ ì•„ì§ ì˜ ëª¨ë¥´ë‹ˆ ë‹¤ì–‘í•œ ë¬¸ì œë“¤ì„ ê³„ì† ë³´ê³ , ì‹¬ì‹¬í•˜ë©´ ë³´ê³ , í•  ê±° ì—†ì„ë•Œ ë³´ê³  ê·¸ëƒ¥ ë§ì´ ë´ë‘ê³  ì°¾ì•„ë³´ëŠ”ê²Œ ë‹µì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
